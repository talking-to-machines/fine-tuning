{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Hugging Face Model (in GGUF Format) to Ollama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Hugging Face Models Downloader package (if not downloaded) and download Hugging Face model to local directory\n",
    "BASE_HUGGINGFACE_MODEL = \"bartowski/Meta-Llama-3.1-70B-Instruct-GGUF\"\n",
    "BASE_OLLAMA_MODEL = \"llama-3.1-70b-instruct\"\n",
    "\n",
    "download_model_command = f\"bash <(curl -sSL https://g.bodaay.io/hfd) -m {BASE_HUGGINGFACE_MODEL}\"\n",
    "!{download_model_command}\n",
    "\n",
    "# Create Modelfile for local model\n",
    "create_modelfile_command = f\"echo 'FROM downloads/{BASE_OLLAMA_MODEL}/{BASE_OLLAMA_MODEL}.gguf' > Modelfile\"\n",
    "!{create_modelfile_command}\n",
    "\n",
    "# Create Ollama model from local model\n",
    "create_model_command = f\"ollama create {BASE_OLLAMA_MODEL} -f Modelfile\"\n",
    "!{create_model_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Hugging Face Model (Original Format) to Ollama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download Hugging Face model locally\n",
    "# iamraymondlow/gemma2-2b-instructiontuned-uncensored\n",
    "BASE_HUGGINGFACE_MODEL = \"iamraymondlow/gemma2-2b-instructiontuned-uncensored\"\n",
    "BASE_OLLAMA_MODEL = \"llama-3.2-3b-instructiontuned-uncensored\"\n",
    "\n",
    "snapshot_download(repo_id=BASE_HUGGINGFACE_MODEL, local_dir=BASE_OLLAMA_MODEL)\n",
    "\n",
    "# Create Modelfile for local model\n",
    "create_modelfile_command = f\"echo 'FROM ./{BASE_OLLAMA_MODEL}' > Modelfile\"\n",
    "!{create_modelfile_command}\n",
    "\n",
    "# Create Ollama model from local model\n",
    "create_model_command = f\"ollama create {BASE_OLLAMA_MODEL} -f Modelfile\"\n",
    "!{create_model_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert local model (Original Format) to GGUF Format to Ollama Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL = \"llama-3.1-8b-instruct-8bit\"\n",
    "\n",
    "# Create Modelfile for local model\n",
    "create_modelfile_command = f\"echo 'FROM ./{LOCAL_MODEL}' > Modelfile\"\n",
    "!{create_modelfile_command}\n",
    "\n",
    "# Convert to GGUF format\n",
    "convert_command = f\"\"\"python llama.cpp/convert_hf_to_gguf.py ./{LOCAL_MODEL} \\\n",
    "  --outfile {LOCAL_MODEL}.gguf \\\n",
    "  --outtype q8_0\"\"\"\n",
    "# convert_command = f\"\"\"python llama.cpp/convert_hf_to_gguf.py ./{LOCAL_MODEL} \\\n",
    "#   --outfile {LOCAL_MODEL}.gguf \"\"\"\n",
    "!{convert_command}\n",
    "\n",
    "# Create Ollama model from local model\n",
    "create_model_command = f\"ollama create {LOCAL_MODEL} -f Modelfile\"\n",
    "!{create_model_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_command = f\"\"\"python llama.cpp/convert_hf_to_gguf.py ./{BASE_OLLAMA_MODEL} \\\n",
    "#   --outfile {BASE_OLLAMA_MODEL}.gguf \\\n",
    "#   --outtype q8_0\"\"\"\n",
    "\n",
    "# !{convert_command}\n",
    "\n",
    "# Create Modelfile for local model\n",
    "# create_modelfile_command = f\"echo 'FROM {BASE_OLLAMA_MODEL}.gguf' > Modelfile\"\n",
    "# !{create_modelfile_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Hugging Face model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
